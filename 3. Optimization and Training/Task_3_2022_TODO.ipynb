{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Task 3 - Optimizers\n",
    "\n",
    "**Requirements:**\n",
    " - numpy (https://numpy.org/)\n",
    " - matplotlib (https://matplotlib.org/)\n",
    "\n",
    "Let's continue with our framework. We use all of the previous implemented classes (with some modifications) and add new - **Optimizers**.\n",
    "\n",
    "Watch out for the shape of input data.. Now we are working with mini-batches $(B,nX,1)$, where $B$ is number of samples in mini-batch, $nX$ is number of features and $1$ is for vector/matrix multiplication in the last 2 dimensions, leaving $B$ as samples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "from utils import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Linear layer (Dense, Fully connected, Single Layer Perceptron)\n",
    "#------------------------------------------------------------------------------\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.W = np.random.randn(out_features, in_features)\n",
    "        self.b = np.zeros((out_features, 1))\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.aPred = input\n",
    "        self.m = self.aPred.shape[0]\n",
    "        net = np.matmul(self.W, input) + self.b\n",
    "        return net\n",
    "\n",
    "    def backward(self, dz: np.ndarray) -> np.ndarray:\n",
    "        self.dW = (1.0/self.m) * np.sum(np.matmul(dz, self.aPred.transpose((0,2,1))), axis=0)\n",
    "        self.db = (1.0/self.m) * np.sum(dz, axis=0)\n",
    "        return np.matmul(self.W.transpose(), dz)\n",
    "\n",
    "    def get_optimizer_context(self):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def set_optimizer_context(self, params):\n",
    "        # TODO\n",
    "        pass\n",
    "#------------------------------------------------------------------------------\n",
    "#   SigmoidActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return 1.0 / (1.0 + np.exp(-input))\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        a = self(self.fw_input)\n",
    "        return np.multiply(da, np.multiply(a, 1 - a))\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   HyperbolicTangentActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return (np.exp(2 * input) - 1) / (np.exp(2 * input) + 1)\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        a = self(self.fw_input)\n",
    "        return np.multiply(da, 1 - np.square(a))\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   Model class\n",
    "#------------------------------------------------------------------------------\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, input) -> np.ndarray:\n",
    "        for name, module in self.modules.items():\n",
    "            input = module(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, z: np.ndarray):\n",
    "        for name, module in reversed(self.modules.items()):\n",
    "            z = module.backward(z)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss Functions\n",
    "\n",
    "As in standard deep learning frameworks, calling Loss function can return either **cost** or  **loss**  based on parameter **reduce**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   MeanSquareErrorLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class MSELoss(Module):\n",
    "    def __init__(self, reduce=\"mean\"):\n",
    "        super(MSELoss, self).__init__()\n",
    "        if reduce == \"mean\":\n",
    "            self.reduce_fn = np.mean\n",
    "        elif reduce == \"sum\":\n",
    "            self.reduce_fn = np.sum\n",
    "        elif reduce is None:\n",
    "            # return identity (do nothing)\n",
    "            self.reduce_fn = lambda x : x\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return self.reduce_fn(np.mean(np.power(target - input, 2), axis=0, keepdims=True))\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return np.mean(-2 * (target - input), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   BinaryCrossEntropyLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class BCELoss(Module):\n",
    "    def __init__(self, reduce=\"mean\"):\n",
    "        super(BCELoss, self).__init__()\n",
    "        if reduce == \"mean\":\n",
    "            self.reduce_fn = np.mean\n",
    "        elif reduce == \"sum\":\n",
    "            self.reduce_fn = np.sum\n",
    "        elif reduce is None:\n",
    "            # return identity (do nothing)\n",
    "            self.reduce_fn = lambda x : x\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return self.reduce_fn(-(target * np.log(input) + np.multiply((1 - target), np.log(1 - input))))\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return -np.divide(target, input) + np.divide(1 - target, 1 - input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizers\n",
    "\n",
    "Each optimizer has as first required parameter **parameters_gen**. It is function for accessing all trainable parameters of Model. Another atributes of optimizer is based on the optimizer definition.\n",
    "\n",
    "Your task is to implement:\n",
    " - SGD with momentum\n",
    " - RMSProp: http://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf\n",
    " - Adam: https://arxiv.org/pdf/1412.6980.pdf\n",
    "\n",
    "All algorithms are in [https://www.deeplearningbook.org/contents/optimization.html](https://www.deeplearningbook.org/contents/optimization.html)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   AbstractOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   StochasticGradientDescentOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, model:Model, lr:float):\n",
    "        super(SGD, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW],[b,db]] = params\n",
    "                    # >>>> start here\n",
    "\n",
    "                    # <<<< end here\n",
    "                    layer.set_optimizer_context([W,b])\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   SGDMomentumOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self, model, lr, momentum):\n",
    "        super(SGDMomentum, self).__init__()\n",
    "        self.model = model\n",
    "                # >>>> start_solution\n",
    "\n",
    "        # <<<< end_solution\n",
    "\n",
    "    def step(self):\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW],[b,db]] = params\n",
    "                    # >>>> start here\n",
    "\n",
    "                    # <<<< end here\n",
    "\n",
    "                    layer.set_optimizer_context([W,b])\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   RMSpropOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class RMSprop(Optimizer):\n",
    "    def __init__(self, model, lr, rho, delta):\n",
    "        super(RMSprop, self).__init__()\n",
    "        self.model = model\n",
    "        # >>>> start_solution\n",
    "\n",
    "        # <<<< end_solution\n",
    "\n",
    "    def step(self):\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW], [b, db]] = params\n",
    "                    # >>>> start here\n",
    "\n",
    "                    # <<<< end here\n",
    "                    layer.set_optimizer_context([W, b])\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   AdamOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, lr, rho1, rho2, delta):\n",
    "        super(Adam, self).__init__()\n",
    "        self.model = model\n",
    "        self.context = {}\n",
    "        # >>>> start_solution\n",
    "\n",
    "        # <<<< end_solution\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW], [b, db]] = params\n",
    "                    # >>>> start here\n",
    "\n",
    "                    # <<<< end here\n",
    "                    layer.set_optimizer_context([W, b])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Processing Cell\n",
    "\n",
    "Watch out for the shape of mini-batch (B,Features,1)\n",
    "\n",
    " 1. Initialize dataset (`dataset_Flower`).\n",
    " 2. Declare a simple model.\n",
    " 3. Initialize optimizer.\n",
    " 4. Make mini-batches.\n",
    " 5. Perform forward pass through the network.\n",
    " 6. Compute loss.\n",
    " 7. Backward prop loss.\n",
    " 8. Track loss.\n",
    " 9. Backward pass MLP.\n",
    " 10. Use optimizer to modify model parameters.\n",
    " 11. Repeat for $N$ epochs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from utils import gradient_check"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from dataset import dataset_Flower, MakeBatches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "dataset = MakeBatches(dataset_Flower(m=512, noise=0.3), 32, True)\n",
    "###>>> start of solution\n",
    "mlp = Model()\n",
    "mlp.add_module(Linear(2, 3), 'Dense_1')\n",
    "mlp.add_module(Tanh(), 'Tanh_1')\n",
    "mlp.add_module(Linear(3, 4), 'Dense_2')\n",
    "mlp.add_module(Tanh(), 'Tanh_2')\n",
    "mlp.add_module(Linear(4, 5), 'Dense_3')\n",
    "mlp.add_module(Tanh(), 'Tanh_3')\n",
    "mlp.add_module(Linear(5, 1), 'Dense_4_out')\n",
    "mlp.add_module(Sigmoid(), 'Sigmoid')\n",
    "loss_fn = MSELoss(reduce='mean')\n",
    "\n",
    "optimizer = SGDMomentum(mlp, lr=0.001, momentum=0.5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "N_epochs = 100\n",
    "losses = []\n",
    "for i in range(N_epochs):\n",
    "    epoch_loss = []\n",
    "    for mini_batch_X, mini_batch_Y in dataset:\n",
    "        predicted_Y_hat = mlp.forward(mini_batch_X)\n",
    "        loss = loss_fn(predicted_Y_hat, mini_batch_Y)\n",
    "        epoch_loss += [np.mean(loss)]\n",
    "        dLoss = loss_fn.backward(predicted_Y_hat, mini_batch_Y)\n",
    "        mlp.backward(dLoss)\n",
    "        # gradient_check(mlp, loss_fn, mini_batch_X, mini_batch_Y)\n",
    "        optimizer.step()\n",
    "    losses += [np.mean(epoch_loss)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = px.line({'SGDMomentum':losses})\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}