{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 2\n",
    "Implement basic backward pass using only numpy:\n",
    "\n",
    "Perform forward pass and backward pass, and use the gradient check function to verify our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import Module"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Layer\n",
    "\n",
    "In your previous task we defined `forward(input)` pass for our Linear class. Now we continue in creation of our own framework a little further with defining the `backward(dNet)` function. The separation of linear unit and activation is beneficial for backward propagation and optimization.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Linear class\n",
    "#------------------------------------------------------------------------------\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.W = np.random.randn(out_features, in_features)\n",
    "        self.dW = np.zeros_like(self.W) # Watch-out for the shape - it has to be same as W\n",
    "        self.b = np.zeros((out_features, 1))\n",
    "        self.db = np.zeros_like(self.b) # Watch-out for the shape - it has to be same as W\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.aPrevious = input\n",
    "        self.m = self.aPred.shape[0]\n",
    "        net = self.W @ input + self.b\n",
    "        return net\n",
    "\n",
    "    def backward(self, dz: np.ndarray) -> np.ndarray:\n",
    "        self.dW = (1.0/self.m) * np.sum(np.matmul(dz, self.aPrevious.transpose((0,2,1))), axis=0)\n",
    "        self.db = (1.0/self.m) * np.sum(dz, axis=0)\n",
    "        return np.matmul(self.W.T, dz)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activations\n",
    "Implement backward pass for Sigmoid, Tanh and ReLU activation functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   SigmoidActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.aPrevious = input\n",
    "        return 1.0 / (1.0 + np.exp(-input))\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        a = self(self.aPrevious)\n",
    "        return np.multiply(da, np.multiply(a, 1 - a))\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   RELUActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.aPrevious = input\n",
    "        return np.maximum(input, 0)\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        return np.multiply(da, (self.aPrevious > 0) * 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss functions\n",
    "For successful backward pass, the computation and derivation of Loss function is necessary.\n",
    "The most common Loss functions are **Mean Square Error** _(MSE, L2)_ **Mean Absolute Error** _(MAE, L1)_ and **Binary Cross Entropy** _(BCE, Log Loss)_ and their modifications according to what is better for the current dataset.\n",
    "\n",
    "Let's implement MSE and BCE Loss functions as Modules of our little framework."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   MeanSquareErrorLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class MSELoss(Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return np.mean(np.power(target - input, 2), axis=0)\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return np.mean(-2 * (target.data - input.data), axis=0)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   BinaryCrossEntropyLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class BCELoss(Module):\n",
    "    def __init__(self):\n",
    "        super(BCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return -(target * np.log(input) + np.multiply((1 - target), np.log(1 - input)))\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return -np.divide(target, input) + np.divide(1 - target, 1 - input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "As in previous task, use `Model` class to encapsulate all layers of our MLP and define backward pass.\n",
    "Iterate over its modules stored in parameter OrderedDict `modules` -> `self.modules` in the correct order.\n",
    "\n",
    "We use call `.add_module(...)` to add layers of our MLP (network).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Model class\n",
    "#------------------------------------------------------------------------------\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, input) -> np.ndarray:\n",
    "        for name, module in self.modules.items():\n",
    "            # print(f'Layer fw:{name}, a.shape = {input.shape} \\n{input}')\n",
    "            input = module(input)\n",
    "            # print(f'z.shape = {input.shape} \\n{input}')\n",
    "        return input\n",
    "\n",
    "    def backward(self, z: np.ndarray):\n",
    "        for name, module in reversed(self.modules.items()):\n",
    "            # print(f'Layer bw {name}, z = \\n{z}')\n",
    "            z = module.backward(z)\n",
    "            # print(f'dZ = \\n{z}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Processing Cell\n",
    "\n",
    " 1. Initialize dataset (`dataset_Circles`).\n",
    " 2. Declare a simple model (at least 3 hidden layer).\n",
    " 3. Perform forward pass through the network.\n",
    " 4. Compute loss.\n",
    " 5. Backward prop loss.\n",
    " 6. Backward pass MLP.\n",
    " 7. Check your computation of gradients via [`gradient_check`](https://datascience-enthusiast.com/DL/Improving_DeepNeural_Networks_Gradient_Checking.html)\n",
    " 8. Start crying.\n",
    " 9. Repeat until correct ;)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from dataset import dataset_circles\n",
    "from utils import gradient_check"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense_1\n",
      "(3, 2)\n",
      "(3, 1)\n",
      "Dense_2\n",
      "(4, 3)\n",
      "(4, 1)\n",
      "Dense_3\n",
      "(5, 4)\n",
      "(5, 1)\n",
      "Dense_4_out\n",
      "(1, 5)\n",
      "(1, 1)\n",
      "\u001B[92mYour backward propagation works perfectly fine! difference = 5.627174213001082e-09\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "dataset_features_X, dataset_labels_Y = dataset_circles(m=128, radius=0.7, noise=0.0)\n",
    "\n",
    "###>>> start of solution\n",
    "mlp = Model()\n",
    "mlp.add_module(Linear(2, 3), 'Dense_1')\n",
    "mlp.add_module(ReLU(), 'Tanh_1')\n",
    "mlp.add_module(Linear(3, 4), 'Dense_2')\n",
    "mlp.add_module(ReLU(), 'Tanh_2')\n",
    "mlp.add_module(Linear(4, 5), 'Dense_3')\n",
    "mlp.add_module(ReLU(), 'Tanh_3')\n",
    "mlp.add_module(Linear(5, 1), 'Dense_4_out')\n",
    "mlp.add_module(Sigmoid(), 'Sigmoid')\n",
    "loss_fn = BCELoss()\n",
    "\n",
    "# losses = []\n",
    "predicted_Y_hat = mlp.forward(dataset_features_X)\n",
    "loss = loss_fn(predicted_Y_hat, dataset_labels_Y)\n",
    "# losses += [np.mean(loss)]\n",
    "dLoss = loss_fn.backward(predicted_Y_hat, dataset_labels_Y)\n",
    "mlp.backward(dLoss)\n",
    "\n",
    "for name, module in mlp.modules.items():\n",
    "    if hasattr(module, 'dW'):\n",
    "        print(name, module.dW.shape, module.db.shape, sep='\\n')\n",
    "\n",
    "gradient_check(mlp, loss_fn, dataset_features_X, dataset_labels_Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}