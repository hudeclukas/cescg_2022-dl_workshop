{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep learning workshop - **Part 1**\n",
    "\n",
    "Let's dive into manual implementation of Neural Network - Multi Layer Perceptron.\n",
    "In this notebook you will:\n",
    "- Implement Linear (also called Dense, Fully-Connected) layer as a Perceptron.\n",
    "- Implement Activation functions to add non-linearity\n",
    "- Allow your solution to stack multiple layers to form MLP network.\n",
    "- Perform forward propagation through your network.\n",
    "\n",
    "This (and later) template implementation is similar to Pytorch framework."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1a:\n",
    "\n",
    "Define a dataset that we can use later for training.\n",
    "Declare a simple perceptron (Linear layer) that inherits defined class Module - it is here, to help you store all network layers.\n",
    "\n",
    "The single layer perceptron should have:\n",
    "1. Weights and Biases for each perceptron\n",
    "..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from collections import OrderedDict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define our dataset\n",
    "Let's make it simple, create a circle dataset where **class 1** is inside of given `radius` and **class 2** is outside given `radius`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def dataset_circles(m=10, radius=0.7, noise=0.0):\n",
    "    # Element values are in the interval <-1; 1>\n",
    "    X = (np.random.rand(m, 2, 1) * 2.0) - 1.0\n",
    "\n",
    "    # Element-wise multiplication with random noise\n",
    "    N = (np.random.rand(m, 2, 1) - 0.5) * noise\n",
    "    Xnoise = X + N\n",
    "\n",
    "    # Compute the radius\n",
    "    # Element-wise square\n",
    "    XSquare = Xnoise ** 2\n",
    "\n",
    "    # Sum over axis=1. We get a (m, 1) array.\n",
    "    RSquare = np.sum(XSquare, axis=1, keepdims=True)\n",
    "    R = np.sqrt(RSquare)\n",
    "\n",
    "    # Y is 1, if radius `R` is greater than `radius`\n",
    "    Y = (R > radius).astype(float)\n",
    "\n",
    "    # Return X, Y\n",
    "    return X, Y\n",
    "\n",
    "def draw_dataset(x, y):\n",
    "    if x.shape[1] == 2:\n",
    "        fig = px.scatter(x=x[:,0,0], y=x[:,1,0], color=y[:,0,0], width=500, height=500, color_continuous_scale='Bluered')\n",
    "    else:\n",
    "        return\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X,Y = dataset_circles(m=100)\n",
    "draw_dataset(X,Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Module\n",
    "\n",
    "All deep learning frameworks have usually one elementary building block.\n",
    "In our project, we follow the structure of the pytorch, so the elementary building block is called **`Module`**.\n",
    "Now, it is pretty simple, but it will get more complex and more useful...\n",
    "You can see function `.backward` that will later contain the partial derivations of chain rule for backward pass and parameter optimization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.modules = OrderedDict()\n",
    "\n",
    "    def add_module(self, module, name:str):\n",
    "        if hasattr(self, name) and name not in self.modules:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"module name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"module name can't be empty string \\\"\\\"\")\n",
    "        self.modules[name] = module\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def backward(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Layer\n",
    "\n",
    "The place, where the mathematical magic happens.\n",
    "To speedup weights multiplication in perceptron it is better to use vectorisation.\n",
    "\n",
    "The activation function and layer logic are separated for easier backward propagation (chain rule) and optimization.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Linear class\n",
    "#------------------------------------------------------------------------------\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        # TODO\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def backward(self, dNet):\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activations\n",
    "\n",
    "The definitions for Sigmoid, ReLU, LeakyReLU, and CELU activation functions with forward and backward pass.\n",
    "Let's start with the forward pass. (for now, we can leave the backward pass on `pass`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   SigmoidActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def backward(self, dNet):\n",
    "        pass\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   RELUActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def backward(self, dNet):\n",
    "        pass\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   ContinuouslyDifferentiableExponentialLinearUnitActivation class\n",
    "#------------------------------------------------------------------------------\n",
    "class CELU(Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(CELU, self).__init__()\n",
    "        self.alpha=alpha\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def backward(self, dNet):\n",
    "        pass\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   LeakyRELUActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class LeakyReLU(Module):\n",
    "    def __init__(self, slope=0.2):\n",
    "        super(LeakyReLU, self).__init__()\n",
    "        self.slope=slope\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def backward(self, dNet):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting the functions\n",
    "Verify our implementations of Activation functions - do the graphs look like they should?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "activationsInput = np.linspace(-4,4,100)\n",
    "\n",
    "sigmoid = Sigmoid()\n",
    "y = sigmoid.forward(activationsInput)\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=activationsInput, y=y, name='Sigmoid'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "tanh = CELU()\n",
    "y = tanh.forward(activationsInput)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=activationsInput, y=y, name='CELU'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "relu = ReLU()\n",
    "y = relu(activationsInput)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=activationsInput, y=y, name='ReLU'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "leakyrelu = LeakyReLU()\n",
    "y = leakyrelu(activationsInput)\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=activationsInput, y=y, name='LeakyReLU'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, width=800, title_text=\"Activation functions\")\n",
    "fig.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Perceptron feed forward\n",
    "\n",
    "Model your Perceptron.\n",
    "Define and initialize perceptron with \"1 neuron\"\n",
    "Feed `X` to the perceptron and see the results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X,Y = dataset_circles(m=4)\n",
    "\n",
    "fc = Linear(X.shape[1],1)\n",
    "print(fc.forward(X))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your Single Layer Perceptron with an Activation function\n",
    "Use previously defined perceptron and use its output as input for the activation function.\n",
    "Feed `X` to the perceptron again and see if something changes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ac = CELU()\n",
    "net = fc.forward(X)\n",
    "net = ac.forward(net)\n",
    "print(net)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1b:\n",
    "\n",
    "Let's implement the `Model` which should contain all of our `Modules` and the call for simple forward feed.\n",
    "\n",
    "### Model class\n",
    "\n",
    "Implementation of the **`Model`** class.\n",
    "Define its forward function - the implementation of forward and backward pass is sensitive to the order of called operations.\n",
    "Each Layer(module) of type **`Module`** can be saved to the attribute **`Module.modules`** using the **`add_module`** method.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Model class\n",
    "#------------------------------------------------------------------------------\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        features = input\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def backward(self, dA: np.ndarray):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 78,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.add_module(Linear(X.shape[1], 4), 'input')\n",
    "model.add_module(ReLU(), 'relu1')\n",
    "model.add_module(Linear(4,5), 'hidden1')\n",
    "model.add_module(LeakyReLU(),'leakyrelu1')\n",
    "model.add_module(Linear(5,1), 'output')\n",
    "model.add_module(Sigmoid(), 'Sigm1')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model.forward(X))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}